{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "!pip install pandas\r\n",
    "!pip install tqdm\r\n",
    "!pip install matplotlib\r\n",
    "!pip install sklearn\r\n",
    "!pip install transformers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with Deep Learning using BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Intermediate-level knowledge of Python 3 (NumPy and Pandas preferably, but not required)\r\n",
    "- Exposure to PyTorch usage\r\n",
    "- Basic understanding of Deep Learning and Language Models (BERT specifically)"
   ],
   "metadata": {
    "collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Project Outline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task 1**: Introduction (this section)\r\n",
    "\r\n",
    "**Task 2**: Exploratory Data Analysis and Preprocessing\r\n",
    "\r\n",
    "**Task 3**: Training/Validation Split\r\n",
    "\r\n",
    "**Task 4**: Loading Tokenizer and Encoding our Data\r\n",
    "\r\n",
    "**Task 5**: Setting up BERT Pretrained Model\r\n",
    "\r\n",
    "**Task 6**: Creating Data Loaders\r\n",
    "\r\n",
    "**Task 7**: Setting Up Optimizer and Scheduler\r\n",
    "\r\n",
    "**Task 8**: Defining our Performance Metrics\r\n",
    "\r\n",
    "**Task 9**: Creating our Training Loop\r\n",
    "\r\n",
    "**Task 10**: Loading and Evaluating our Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1: Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is BERT\r\n",
    "\r\n",
    "BERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks.\r\n",
    "\r\n",
    "For more information, the original paper can be found [here](https://arxiv.org/abs/1810.04805). \r\n",
    "\r\n",
    "[HuggingFace documentation](https://huggingface.co/transformers/model_doc/bert.html)\r\n",
    "\r\n",
    "[Bert documentation](https://characters.fandom.com/wiki/Bert_(Sesame_Street) ;)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"Images/BERT_diagrams.png\" width=\"1000\">"
   ],
   "metadata": {
    "collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2: Exploratory Data Analysis and Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the SMILE Twitter dataset.\n",
    "\n",
    "_Wang, Bo; Tsakalidis, Adam; Liakata, Maria; Zubiaga, Arkaitz; Procter, Rob; Jensen, Eric (2016): SMILE Twitter Emotion dataset. figshare. Dataset. https://doi.org/10.6084/m9.figshare.3187909.v2_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import pandas as pd\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"./data/smile-annotations-final.csv\", names= [\"id\",\"text\",\"category\"])\r\n",
    "data.set_index(\"id\", inplace=True)\r\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.category.value_counts().plot(kind=\"bar\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_cleaned = data.loc[\r\n",
    "    ~(data[\"category\"].str.contains('[|]')\r\n",
    "    | data[\"category\"].str.contains('(nocode)'))\r\n",
    "    ]\r\n",
    "\r\n",
    "data_cleaned.category.value_counts().plot(kind=\"bar\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "possible_labels = data_cleaned.category.unique()\r\n",
    "labels_dict = {}\r\n",
    "for index, label in enumerate(possible_labels):\r\n",
    "    labels_dict[label] = index"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_cleaned[\"label\"] = data_cleaned.category.map(labels_dict)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3: Training/Validation Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train, df_test, y_train, y_test = train_test_split(\r\n",
    "    data_cleaned.drop([\"label\",\"category\"], axis=1), \r\n",
    "    data_cleaned[\"label\"], \r\n",
    "    test_size=0.15, \r\n",
    "    random_state=42, \r\n",
    "    stratify=data_cleaned[\"label\"]\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_cleaned[\"data_type\"] = \"not_set\"\r\n",
    "data_cleaned.loc[df_train.index, \"data_type\"] = \"train\"\r\n",
    "data_cleaned.loc[df_test.index, \"data_type\"] = \"val\"\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_cleaned.groupby([\"category\",\"label\",\"data_type\"]).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4: Loading Tokenizer and Encoding our Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\r\n",
    "from torch.utils.data import TensorDataset"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\r\n",
    "    \"bert-base-uncased\",\r\n",
    "    do_lower_case = True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\r\n",
    "    data_cleaned[data_cleaned.data_type==\"train\"].text.values,\r\n",
    "    add_special_tokens=True, # add special tokens such as [EOS,CLS]\r\n",
    "    return_attention_mask=True,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    max_length=256,\r\n",
    "    return_tensors=\"pt\"\r\n",
    "\r\n",
    ")\r\n",
    "\r\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\r\n",
    "    data_cleaned[data_cleaned.data_type==\"val\"].text.values,\r\n",
    "    add_special_tokens=True,\r\n",
    "    return_attention_mask=True,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    max_length=256,\r\n",
    "    return_tensors=\"pt\"\r\n",
    "\r\n",
    ")\r\n",
    "\r\n",
    "input_ids_train = encoded_data_train[\"input_ids\"]\r\n",
    "attention_masks_train = encoded_data_train[\"attention_mask\"]\r\n",
    "labels_train = torch.tensor(data_cleaned[data_cleaned.data_type==\"train\"].label.values)\r\n",
    "\r\n",
    "input_ids_val = encoded_data_val[\"input_ids\"]\r\n",
    "attention_masks_val = encoded_data_val[\"attention_mask\"]\r\n",
    "labels_val = torch.tensor(data_cleaned[data_cleaned.data_type==\"val\"].label.values)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\r\n",
    "\r\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 5: Setting up BERT Pretrained Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertForSequenceClassification"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\r\n",
    "    \"bert-base-uncased\",\r\n",
    "    num_labels= len(labels_dict),\r\n",
    "    output_attentions=False,\r\n",
    "    output_hidden_states=False\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 6: Creating Data Loaders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 4\r\n",
    "\r\n",
    "data_loader_train = DataLoader(\r\n",
    "    dataset_train,\r\n",
    "    sampler=RandomSampler(dataset_train),\r\n",
    "    batch_size=batch_size\r\n",
    ")\r\n",
    "\r\n",
    "data_loader_val = DataLoader(\r\n",
    "    dataset_val,\r\n",
    "    sampler=SequentialSampler(dataset_val),\r\n",
    "    batch_size=batch_size\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 7: Setting Up Optimizer and Scheduler"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = AdamW(\r\n",
    "    model.parameters(),\r\n",
    "    lr=1e-5, #2e-5<5e-5\r\n",
    "    eps=1e-8\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 10\r\n",
    "\r\n",
    "scheduler = get_linear_schedule_with_warmup(\r\n",
    "    optimizer,\r\n",
    "    num_warmup_steps = 0,\r\n",
    "    num_training_steps = len(data_loader_train)*epochs\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 8: Defining our Performance Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy metric approach originally used in accuracy function in [this tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def f1_score_func(preds, labels):\r\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\r\n",
    "    labels_flat = labels.flatten()\r\n",
    "\r\n",
    "    return f1_score(labels_flat, preds_flat, average=\"weighted\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracy_per_class(preds, labels):\r\n",
    "    label_dict_inverse = {v: k for k,v in labels_dict.items()}\r\n",
    "    \r\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\r\n",
    "    labels_flat = labels.flatten()\r\n",
    "\r\n",
    "    for label in np.unique(labels_flat):\r\n",
    "        y_preds = preds_flat[preds_flat==label]\r\n",
    "        y_true = labels_flat[labels_flat==label]\r\n",
    "        print(f'Class: {label_dict_inverse[label]}')\r\n",
    "        print(f\"Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n\")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 9: Creating our Training Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Approach adapted from an older version of HuggingFace's `run_glue.py` script. Accessible [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "\r\n",
    "seed_val = 17\r\n",
    "random.seed(seed_val)\r\n",
    "np.random.seed(seed_val)\r\n",
    "torch.manual_seed(seed_val)\r\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\")\r\n",
    "model.to(device)\r\n",
    "print(\"ok\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(dataloader_val):\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    loss_val_total = 0\r\n",
    "    predictions, true_vals = [], []\r\n",
    "    \r\n",
    "    for batch in tqdm(dataloader_val):\r\n",
    "        \r\n",
    "        batch = tuple(b.to(device) for b in batch)\r\n",
    "        \r\n",
    "        inputs = {'input_ids':      batch[0],\r\n",
    "                  'attention_mask': batch[1],\r\n",
    "                  'labels':         batch[2],\r\n",
    "                 }\r\n",
    "\r\n",
    "        with torch.no_grad():        \r\n",
    "            outputs = model(**inputs)\r\n",
    "            \r\n",
    "        loss = outputs[0]\r\n",
    "        logits = outputs[1]\r\n",
    "        loss_val_total += loss.item()\r\n",
    "\r\n",
    "        logits = logits.detach().cpu().numpy()\r\n",
    "        label_ids = inputs['labels'].cpu().numpy()\r\n",
    "        predictions.append(logits)\r\n",
    "        true_vals.append(label_ids)\r\n",
    "    \r\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \r\n",
    "    \r\n",
    "    predictions = np.concatenate(predictions, axis=0)\r\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\r\n",
    "            \r\n",
    "    return loss_val_avg, predictions, true_vals\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    loss_train_total = 0\r\n",
    "\r\n",
    "    progress_bar = tqdm(\r\n",
    "        data_loader_train, \r\n",
    "        desc=f\"Epoch: {epoch}\",\r\n",
    "        leave=False,\r\n",
    "        disable=False\r\n",
    "        )\r\n",
    "    for batch in progress_bar:\r\n",
    "        model.zero_grad()\r\n",
    "        batch = tuple(b.to(device) for b in batch)\r\n",
    "\r\n",
    "        inputs = {\r\n",
    "            \"input_ids\"     : batch[0],\r\n",
    "            \"attention_mask\": batch[1],\r\n",
    "            \"labels\"        : batch[2]\r\n",
    "\r\n",
    "        }\r\n",
    "\r\n",
    "        outputs = model(**inputs)\r\n",
    "\r\n",
    "        loss = outputs[0]\r\n",
    "        loss_train_total += float(loss.item())\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
    "\r\n",
    "        optimizer.step()\r\n",
    "        scheduler.step()\r\n",
    "\r\n",
    "        progress_bar.set_postfix({\"training_loss\": \"{:.3f}\".format(loss.item()/len(batch))})\r\n",
    "    torch.save(model.state_dict(), f\"Models/BERT_ft_epoch{epoch}.model\")\r\n",
    "    tqdm.write(f\"\\nEpoch {epoch}\")\r\n",
    "\r\n",
    "    loss_train_avg = loss_train_total/len(data_loader_train)\r\n",
    "\r\n",
    "    tqdm.write(f\"Training loss: {loss_train_avg}\")\r\n",
    "\r\n",
    "    val_loss, predictions, true_vals = evaluate(data_loader_val)\r\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\r\n",
    "\r\n",
    "    tqdm.write(f\"Valisation loss: {val_loss}\")\r\n",
    "    tqdm.write(f\"F1 score: {val_f1}\")\r\n",
    "\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 10: Loading and Evaluating our Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\r\n",
    "                                                      num_labels=len(labels_dict),\r\n",
    "                                                      output_attentions=False,\r\n",
    "                                                      output_hidden_states=False)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_state_dict(\r\n",
    "    torch.load(\"Models/BERT_ft_epoch9.model\" , map_location=torch.device(\"cuda\"))\r\n",
    "    \r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, predictions, true_vals = evaluate(data_loader_val)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "020a204bcd438bb21a84599d653c6fbec771eaea1a728476542434418412d72c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}